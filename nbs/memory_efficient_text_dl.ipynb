{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-Efficient Text DataLoader\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Unroll and pre-compute numericalized text data\n",
    "2. Store it efficiently to disk using numpy memmap\n",
    "3. Create a custom DataLoader that streams from disk\n",
    "4. Configure number of workers to reduce memory usage\n",
    "\n",
    "This approach fixes memory leaks in the standard LMDataLoader by avoiding:\n",
    "- ReindexCollection caching issues\n",
    "- Repeated reshuffling and chunk recreation\n",
    "- High memory usage with many workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.basics import *\n",
    "from fastai.callback.all import *\n",
    "from fastai.text.all import *\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download Wikipedia Tiny Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_train = pd.read_csv(path/'train.csv', header=None)\n",
    "df_valid = pd.read_csv(path/'test.csv', header=None)\n",
    "df_all = pd.concat([df_train, df_valid])\n",
    "print(f\"Train samples: {len(df_train)}, Valid samples: {len(df_valid)}, Total: {len(df_all)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenize and Numericalize Text\n",
    "\n",
    "First, we'll process the text through the standard fastai pipeline to create vocabulary and numericalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits\n",
    "splits = [list(range_of(df_train)), list(range(len(df_train), len(df_all)))]\n",
    "print(f\"Train indices: 0-{splits[0][-1]}, Valid indices: {splits[1][0]}-{splits[1][-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transforms pipeline\n",
    "# Note: We use Tokenizer.from_df for dataframe input\n",
    "tok = Tokenizer.from_df(0)  # Column 0 contains text\n",
    "num = Numericalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, tokenize all texts to build vocabulary\n",
    "# This is the standard approach using Datasets\n",
    "tfms = [attrgetter(\"text\"), tok, num]\n",
    "dsets = Datasets(df_all, [tfms], splits=splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary from the numericalize transform\n",
    "vocab = num.vocab\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"First 20 tokens: {vocab[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unroll Dataset to Disk Storage\n",
    "\n",
    "Now we'll extract all numericalized tokens and store them efficiently using numpy memmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiskLMDataset:\n",
    "    \"\"\"\n",
    "    Stores numericalized text data on disk using numpy memmap.\n",
    "    \n",
    "    This avoids memory leaks by:\n",
    "    1. Pre-computing all tokens once\n",
    "    2. Storing as contiguous array on disk\n",
    "    3. Memory-mapping for efficient access without loading all data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Path):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.data_path = self.cache_dir / 'tokens.npy'\n",
    "        self.meta_path = self.cache_dir / 'meta.json'\n",
    "        self.vocab_path = self.cache_dir / 'vocab.pkl'\n",
    "        \n",
    "        self._data = None\n",
    "        self._meta = None\n",
    "        self._vocab = None\n",
    "        \n",
    "    @classmethod\n",
    "    def from_datasets(cls, dsets, cache_dir: Path, split_idx: int = 0, \n",
    "                      force_rebuild: bool = False, vocab=None):\n",
    "        \"\"\"\n",
    "        Create a DiskLMDataset from a fastai Datasets object.\n",
    "        \n",
    "        Args:\n",
    "            dsets: fastai Datasets with numericalized text\n",
    "            cache_dir: Directory to store cached data\n",
    "            split_idx: Which split to use (0=train, 1=valid)\n",
    "            force_rebuild: If True, rebuild even if cache exists\n",
    "            vocab: Vocabulary list to save (pass from Numericalize transform)\n",
    "        \"\"\"\n",
    "        obj = cls(cache_dir)\n",
    "        \n",
    "        # Check if cache exists\n",
    "        if not force_rebuild and obj.data_path.exists() and obj.meta_path.exists():\n",
    "            print(f\"Loading from cache: {cache_dir}\")\n",
    "            obj._load_cache()\n",
    "            return obj\n",
    "        \n",
    "        print(f\"Building cache in: {cache_dir}\")\n",
    "        \n",
    "        # Get the specific split\n",
    "        split_dset = dsets.subset(split_idx)\n",
    "        \n",
    "        # Collect all numericalized tokens\n",
    "        all_tokens = []\n",
    "        doc_lengths = []\n",
    "        \n",
    "        print(\"Extracting tokens...\")\n",
    "        for i in progress_bar(range(len(split_dset))):\n",
    "            item = split_dset[i]\n",
    "            # Handle tuple (x,) or just x\n",
    "            tokens = item[0] if isinstance(item, tuple) else item\n",
    "            tokens = tokens.numpy() if hasattr(tokens, 'numpy') else np.array(tokens)\n",
    "            all_tokens.append(tokens)\n",
    "            doc_lengths.append(len(tokens))\n",
    "        \n",
    "        # Concatenate all tokens\n",
    "        print(\"Concatenating tokens...\")\n",
    "        all_tokens = np.concatenate(all_tokens).astype(np.int32)\n",
    "        total_tokens = len(all_tokens)\n",
    "        \n",
    "        print(f\"Total tokens: {total_tokens:,}\")\n",
    "        \n",
    "        # Save to disk\n",
    "        print(\"Saving to disk...\")\n",
    "        np.save(obj.data_path, all_tokens)\n",
    "        \n",
    "        # Save metadata\n",
    "        meta = {\n",
    "            'total_tokens': total_tokens,\n",
    "            'doc_lengths': doc_lengths,\n",
    "            'num_docs': len(doc_lengths),\n",
    "        }\n",
    "        with open(obj.meta_path, 'w') as f:\n",
    "            json.dump(meta, f)\n",
    "        \n",
    "        # Save vocabulary if provided\n",
    "        if vocab is not None:\n",
    "            save_pickle(obj.vocab_path, vocab)\n",
    "        \n",
    "        obj._load_cache()\n",
    "        print(f\"Cache built successfully!\")\n",
    "        return obj\n",
    "    \n",
    "    def _load_cache(self):\n",
    "        \"\"\"Load cached data using memory mapping.\"\"\"\n",
    "        self._data = np.load(self.data_path, mmap_mode='r')\n",
    "        with open(self.meta_path, 'r') as f:\n",
    "            self._meta = json.load(f)\n",
    "        if self.vocab_path.exists():\n",
    "            self._vocab = load_pickle(self.vocab_path)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        if self._data is None:\n",
    "            self._load_cache()\n",
    "        return self._data\n",
    "    \n",
    "    @property\n",
    "    def meta(self):\n",
    "        if self._meta is None:\n",
    "            self._load_cache()\n",
    "        return self._meta\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        if self._vocab is None:\n",
    "            self._load_cache()\n",
    "        return self._vocab\n",
    "    \n",
    "    @property\n",
    "    def total_tokens(self):\n",
    "        return self.meta['total_tokens']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_tokens\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get token(s) at index. Supports slicing.\"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create disk datasets for train and valid\n",
    "cache_base = path / 'lm_cache'\n",
    "\n",
    "train_disk = DiskLMDataset.from_datasets(dsets, cache_base / 'train', split_idx=0, vocab=vocab)\n",
    "valid_disk = DiskLMDataset.from_datasets(dsets, cache_base / 'valid', split_idx=1, vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train tokens: {train_disk.total_tokens:,}\")\n",
    "print(f\"Valid tokens: {valid_disk.total_tokens:,}\")\n",
    "print(f\"Vocab size: {len(train_disk.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "print(\"First 20 tokens:\", train_disk[:20])\n",
    "print(\"Decoded:\", [train_disk.vocab[i] for i in train_disk[:20]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Memory-Efficient DataLoader\n",
    "\n",
    "Now we create a custom DataLoader that:\n",
    "1. Reads from the disk-cached data\n",
    "2. Uses minimal memory\n",
    "3. Supports configurable number of workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiskLMDataLoaderDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset that reads from DiskLMDataset.\n",
    "    \n",
    "    Implements the same chunking logic as LMDataLoader but reads from disk.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, disk_data: DiskLMDataset, bs: int = 64, seq_len: int = 72):\n",
    "        self.disk_data = disk_data\n",
    "        self.bs = bs\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Calculate dimensions (same logic as LMDataLoader)\n",
    "        total = disk_data.total_tokens - 1  # -1 for labels\n",
    "        self.corpus = round_multiple(total, bs, round_down=True)\n",
    "        self.bl = self.corpus // bs  # batch length\n",
    "        self.n_batches = self.bl // seq_len + int(self.bl % seq_len != 0)\n",
    "        self.last_len = self.bl - (self.n_batches - 1) * seq_len\n",
    "        \n",
    "        # For shuffling, we create a permutation of batch indices\n",
    "        self._batch_order = None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_batches * self.bs\n",
    "    \n",
    "    def shuffle(self):\n",
    "        \"\"\"Create new random batch ordering for this epoch.\"\"\"\n",
    "        # We shuffle at the batch level, not token level\n",
    "        # This maintains coherent sequences within batches\n",
    "        self._batch_order = torch.randperm(self.n_batches).numpy()\n",
    "    \n",
    "    def __getitem__(self, seq):\n",
    "        \"\"\"\n",
    "        Get a single training example (input, target).\n",
    "        \n",
    "        seq: linear index into all sequences\n",
    "        \"\"\"\n",
    "        if seq >= len(self):\n",
    "            raise IndexError(f\"Index {seq} out of range for dataset of size {len(self)}\")\n",
    "        \n",
    "        # Determine which position in the stream this is\n",
    "        batch_idx = seq // self.bs  # which batch (0 to n_batches-1)\n",
    "        stream_idx = seq % self.bs   # which stream within the batch (0 to bs-1)\n",
    "        \n",
    "        # Apply batch shuffling if set\n",
    "        if self._batch_order is not None:\n",
    "            batch_idx = self._batch_order[batch_idx]\n",
    "        \n",
    "        # Calculate sequence length for this batch\n",
    "        sl = self.last_len if batch_idx == self.n_batches - 1 else self.seq_len\n",
    "        \n",
    "        # Calculate start position in the token stream\n",
    "        # Each stream is bl tokens long, starts at stream_idx * bl\n",
    "        # Within the stream, we're at batch_idx * seq_len\n",
    "        start = stream_idx * self.bl + batch_idx * self.seq_len\n",
    "        \n",
    "        # Get tokens (sl+1 to have input and target)\n",
    "        tokens = self.disk_data[start:start + sl + 1].copy()\n",
    "        tokens = torch.from_numpy(tokens).long()\n",
    "        \n",
    "        x = LMTensorText(tokens[:-1])\n",
    "        y = tokens[1:]\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientLMDataLoader:\n",
    "    \"\"\"\n",
    "    Memory-efficient Language Model DataLoader.\n",
    "    \n",
    "    Key differences from standard LMDataLoader:\n",
    "    1. Reads from disk-cached data (no in-memory caching issues)\n",
    "    2. Configurable num_workers (default 0 to avoid multiprocessing memory)\n",
    "    3. Optional batch-level shuffling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 disk_data: DiskLMDataset,\n",
    "                 bs: int = 64,\n",
    "                 seq_len: int = 72,\n",
    "                 num_workers: int = 0,\n",
    "                 shuffle: bool = True,\n",
    "                 drop_last: bool = True,\n",
    "                 pin_memory: bool = False):\n",
    "        \n",
    "        self.disk_data = disk_data\n",
    "        self.bs = bs\n",
    "        self.seq_len = seq_len\n",
    "        self.num_workers = num_workers\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.pin_memory = pin_memory\n",
    "        \n",
    "        # Create underlying dataset\n",
    "        self.dataset = DiskLMDataLoaderDataset(disk_data, bs=bs, seq_len=seq_len)\n",
    "        \n",
    "        # Store vocab for decode\n",
    "        self.vocab = disk_data.vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.n_batches\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over batches.\"\"\"\n",
    "        if self.shuffle:\n",
    "            self.dataset.shuffle()\n",
    "        \n",
    "        # Create DataLoader\n",
    "        dl = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=self.bs,\n",
    "            shuffle=False,  # We handle shuffling ourselves\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=self.pin_memory,\n",
    "            drop_last=self.drop_last,\n",
    "            collate_fn=self._collate\n",
    "        )\n",
    "        \n",
    "        for batch in dl:\n",
    "            yield batch\n",
    "    \n",
    "    def _collate(self, batch):\n",
    "        \"\"\"Collate a batch of (x, y) pairs.\"\"\"\n",
    "        xs, ys = zip(*batch)\n",
    "        # Stack into batch tensors\n",
    "        x = torch.stack([x for x in xs])\n",
    "        y = torch.stack([y for y in ys])\n",
    "        return x, y\n",
    "    \n",
    "    def one_batch(self):\n",
    "        \"\"\"Get one batch for testing.\"\"\"\n",
    "        return next(iter(self))\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        \"\"\"Decode tokens to text.\"\"\"\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.cpu().numpy()\n",
    "        return ' '.join([self.vocab[t] for t in tokens])\n",
    "    \n",
    "    def show_batch(self, max_n: int = 3):\n",
    "        \"\"\"Display a few examples from one batch.\"\"\"\n",
    "        x, y = self.one_batch()\n",
    "        for i in range(min(max_n, len(x))):\n",
    "            print(f\"--- Example {i+1} ---\")\n",
    "            print(f\"Input:  {self.decode(x[i][:50])}...\")\n",
    "            print(f\"Target: {self.decode(y[i][:50])}...\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create memory-efficient dataloaders\n",
    "bs, sl = 64, 72\n",
    "\n",
    "train_dl = MemoryEfficientLMDataLoader(\n",
    "    train_disk, \n",
    "    bs=bs, \n",
    "    seq_len=sl, \n",
    "    num_workers=0,  # Use 0 workers to minimize memory\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_dl = MemoryEfficientLMDataLoader(\n",
    "    valid_disk, \n",
    "    bs=bs, \n",
    "    seq_len=sl, \n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train batches: {len(train_dl)}\")\n",
    "print(f\"Valid batches: {len(valid_dl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a batch\n",
    "train_dl.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify batch shapes\n",
    "x, y = train_dl.one_batch()\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with fastai Learner\n",
    "\n",
    "Now let's create a wrapper that makes this work with fastai's training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiskLMDataLoaders(DataLoaders):\n",
    "    \"\"\"\n",
    "    DataLoaders wrapper for memory-efficient LM training.\n",
    "    \n",
    "    Works with fastai's Learner.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, train_dl, valid_dl, vocab, device=None):\n",
    "        self.loaders = [train_dl, valid_dl]\n",
    "        self._vocab = vocab\n",
    "        self.device = default_device() if device is None else device\n",
    "    \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.loaders[0]\n",
    "    \n",
    "    @property\n",
    "    def valid(self):\n",
    "        return self.loaders[1]\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self._vocab\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.loaders)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.loaders)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.loaders[i]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_cache(cls, cache_dir: Path, bs: int = 64, seq_len: int = 72, \n",
    "                   num_workers: int = 0, device=None):\n",
    "        \"\"\"\n",
    "        Create DataLoaders from cached disk data.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir: Base cache directory (should have 'train' and 'valid' subdirs)\n",
    "            bs: Batch size\n",
    "            seq_len: Sequence length\n",
    "            num_workers: Number of dataloader workers (0 recommended for low memory)\n",
    "            device: PyTorch device\n",
    "        \"\"\"\n",
    "        cache_dir = Path(cache_dir)\n",
    "        \n",
    "        # Load disk datasets\n",
    "        train_disk = DiskLMDataset(cache_dir / 'train')\n",
    "        valid_disk = DiskLMDataset(cache_dir / 'valid')\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dl = MemoryEfficientLMDataLoader(\n",
    "            train_disk, bs=bs, seq_len=seq_len, \n",
    "            num_workers=num_workers, shuffle=True\n",
    "        )\n",
    "        valid_dl = MemoryEfficientLMDataLoader(\n",
    "            valid_disk, bs=bs, seq_len=seq_len,\n",
    "            num_workers=num_workers, shuffle=False\n",
    "        )\n",
    "        \n",
    "        return cls(train_dl, valid_dl, train_disk.vocab, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "dls = DiskLMDataLoaders.from_cache(\n",
    "    path / 'lm_cache',\n",
    "    bs=64,\n",
    "    seq_len=72,\n",
    "    num_workers=0  # Zero workers for minimal memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Vocab size: {len(dls.vocab)}\")\n",
    "print(f\"Train batches: {len(dls.train)}\")\n",
    "print(f\"Valid batches: {len(dls.valid)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create and Train a Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple training loop compatible with our DataLoaders\n",
    "import gc\n",
    "\n",
    "def train_epoch(model, dl, loss_func, opt, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for x, y in progress_bar(dl):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        out = model(x)[0]  # AWD-LSTM returns (output, hidden, dropped_output)\n",
    "        loss = loss_func(out.view(-1, out.shape[-1]), y.view(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        # Explicit garbage collection to help with memory\n",
    "        del x, y, out, loss\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return total_loss / n_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dl, loss_func, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for x, y in dl:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)[0]\n",
    "        loss = loss_func(out.view(-1, out.shape[-1]), y.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    return total_loss / n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "config = awd_lstm_lm_config.copy()\n",
    "config.update({'input_p': 0.6, 'output_p': 0.4, 'weight_p': 0.5, 'embed_p': 0.1, 'hidden_p': 0.2})\n",
    "\n",
    "model = get_language_model(AWD_LSTM, len(dls.vocab), config=config)\n",
    "device = default_device()\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model on: {device}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer and loss\n",
    "opt = Adam(model.parameters(), lr=5e-3, wd=0.1)\n",
    "loss_func = CrossEntropyLossFlat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for a few epochs\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_epoch(model, dls.train, loss_func, opt, device)\n",
    "    valid_loss = validate(model, dls.valid, loss_func, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: train_loss={train_loss:.4f}, valid_loss={valid_loss:.4f}, perplexity={np.exp(valid_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Memory Usage Comparison\n",
    "\n",
    "Let's compare memory usage between standard and disk-based approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_mb():\n",
    "    \"\"\"Get current process memory in MB.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "print(f\"Current memory usage: {get_memory_mb():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test iteration without memory leaks\n",
    "gc.collect()\n",
    "mem_before = get_memory_mb()\n",
    "\n",
    "# Iterate through multiple epochs\n",
    "for epoch in range(3):\n",
    "    for batch_idx, (x, y) in enumerate(dls.train):\n",
    "        if batch_idx >= 10:  # Just test a few batches\n",
    "            break\n",
    "        del x, y\n",
    "    gc.collect()\n",
    "    print(f\"Epoch {epoch+1}: Memory = {get_memory_mb():.1f} MB\")\n",
    "\n",
    "mem_after = get_memory_mb()\n",
    "print(f\"\\nMemory change: {mem_after - mem_before:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook demonstrates a memory-efficient approach for language model training:\n",
    "\n",
    "### Key Components:\n",
    "1. **DiskLMDataset**: Stores numericalized tokens on disk using numpy memmap\n",
    "2. **DiskLMDataLoaderDataset**: PyTorch Dataset that reads from disk cache\n",
    "3. **MemoryEfficientLMDataLoader**: Custom DataLoader with configurable workers\n",
    "4. **DiskLMDataLoaders**: fastai-compatible DataLoaders wrapper\n",
    "\n",
    "### Benefits:\n",
    "- **Reduced memory**: Data stays on disk, only loaded when needed\n",
    "- **No memory leaks**: Avoids ReindexCollection caching issues\n",
    "- **Configurable workers**: Use `num_workers=0` for minimal memory\n",
    "- **Fast second load**: Pre-computed cache loads instantly\n",
    "- **Large dataset support**: Can handle datasets larger than RAM\n",
    "\n",
    "### Usage:\n",
    "```python\n",
    "# First time: Build cache (pass vocab from Numericalize transform)\n",
    "train_disk = DiskLMDataset.from_datasets(dsets, cache_dir / 'train', split_idx=0, vocab=num.vocab)\n",
    "\n",
    "# Second time: Load from cache (fast!)\n",
    "dls = DiskLMDataLoaders.from_cache(cache_dir, bs=64, seq_len=72, num_workers=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
